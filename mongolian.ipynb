{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca93159b",
   "metadata": {},
   "outputs": [],
   "source": [
   
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import difflib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7618b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def normalize_form(s: object) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    if isinstance(s, float) and np.isnan(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"’\", \"'\").replace(\"ʻ\", \"'\").replace(\"`\", \"'\").replace(\"´\", \"'\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def strip_diacritics(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "_CYR2LAT = {\n",
    "    \"а\":\"a\",\"б\":\"b\",\"в\":\"v\",\"г\":\"g\",\"д\":\"d\",\"е\":\"e\",\"ё\":\"yo\",\"ж\":\"zh\",\"з\":\"z\",\"и\":\"i\",\"й\":\"y\",\n",
    "    \"к\":\"k\",\"л\":\"l\",\"м\":\"m\",\"н\":\"n\",\"о\":\"o\",\"п\":\"p\",\"р\":\"r\",\"с\":\"s\",\"т\":\"t\",\"у\":\"u\",\"ф\":\"f\",\n",
    "    \"х\":\"kh\",\"ц\":\"ts\",\"ч\":\"ch\",\"ш\":\"sh\",\"щ\":\"shch\",\"ъ\":\"\",\"ы\":\"y\",\"ь\":\"\",\"э\":\"e\",\"ю\":\"yu\",\"я\":\"ya\",\n",
    "    # монгольские кириллические\n",
    "    \"ө\":\"o\",\"ү\":\"u\",\"ң\":\"ng\",\"һ\":\"h\",\n",
    "}\n",
    "\n",
    "def cyrillic_to_latin(s: str) -> str:\n",
    "    s = normalize_form(s).lower()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        out.append(_CYR2LAT.get(ch, ch))\n",
    "    return \"\".join(out)\n",
    "\n",
    "def romanize_for_alignment(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Единое представление для сравнения разных алфавитов:\n",
    "    - если есть кириллица -> простая латинизация\n",
    "    - затем удаление диакритики (xiào -> xiao)\n",
    "    \"\"\"\n",
    "    s = normalize_form(s)\n",
    "    # если в строке есть кириллица, латинизируем\n",
    "    if re.search(r\"[А-Яа-яЁёӨөҮүҢңҺһ]\", s):\n",
    "        s = cyrillic_to_latin(s)\n",
    "    s = s.lower()\n",
    "    s = strip_diacritics(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def parse_pairs_txt(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ожидаемый формат строк:\n",
    "      form1 gloss1 form2 gloss2 1\n",
    "    или\n",
    "      form1 form2 1\n",
    "    или (для text2)\n",
    "      ... 0\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().replace(\"\\ufeff\", \"\")\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = re.split(r\"\\s+\", line)\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "\n",
    "            if re.fullmatch(r\"[01]\", parts[-1]):\n",
    "                label = int(parts[-1])\n",
    "                core = parts[:-1]\n",
    "            else:\n",
    "                label = 1\n",
    "                core = parts\n",
    "\n",
    "            if len(core) == 2:\n",
    "                form1, form2 = core\n",
    "                gloss1, gloss2 = \"\", \"\"\n",
    "            elif len(core) == 3:\n",
    "                form1, form2, gloss2 = core\n",
    "                gloss1 = \"\"\n",
    "            else:\n",
    "                form1 = core[0]\n",
    "                gloss1 = core[1]\n",
    "                form2 = core[2]\n",
    "                gloss2 = \" \".join(core[3:])\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"form1\": normalize_form(form1),\n",
    "                    \"form2\": normalize_form(form2),\n",
    "                    \"gloss1\": gloss1,\n",
    "                    \"gloss2\": gloss2,\n",
    "                    \"label\": label,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_id_map(series: pd.Series) -> Dict[str, int]:\n",
    "    uniq = sorted(set(series.dropna().astype(str).tolist()))\n",
    "    return {v: i for i, v in enumerate(uniq)}\n",
    "\n",
    "\n",
    "def compute_features(src_form: str, tgt_form: str) -> np.ndarray:\n",
    "    a = normalize_form(src_form)\n",
    "    b = normalize_form(tgt_form)\n",
    "    la, lb = len(a), len(b)\n",
    "    minl = max(1, min(la, lb))\n",
    "    maxl = max(1, max(la, lb))\n",
    "\n",
    "    ratio = difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    pref = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if x == y:\n",
    "            pref += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    suf = 0\n",
    "    for x, y in zip(a[::-1], b[::-1]):\n",
    "        if x == y:\n",
    "            suf += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    pref_r = pref / minl\n",
    "    suf_r = suf / minl\n",
    "\n",
    "    def bigrams(s: str) -> set:\n",
    "        return set([s[i : i + 2] for i in range(len(s) - 1)]) if len(s) >= 2 else set()\n",
    "\n",
    "    A = bigrams(a)\n",
    "    B = bigrams(b)\n",
    "    jac = (len(A & B) / max(1, len(A | B))) if (A or B) else 0.0\n",
    "\n",
    "    feat = np.array(\n",
    "        [\n",
    "            la,\n",
    "            lb,\n",
    "            abs(la - lb),\n",
    "            la / maxl,\n",
    "            lb / maxl,\n",
    "            ratio,\n",
    "            pref_r,\n",
    "            suf_r,\n",
    "            jac,\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b110dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Encoding (byte-level + meta)\n",
    "# =========================\n",
    "\n",
    "PAD_ID = 0\n",
    "CLS_ID = 1\n",
    "SEP_ID = 2\n",
    "BYTE_OFFSET = 3  # bytes 0..255 -> 3..258\n",
    "META_OFFSET = BYTE_OFFSET + 256  # 259\n",
    "\n",
    "\n",
    "def to_bytes_ids(s: str, max_bytes: int) -> List[int]:\n",
    "    b = normalize_form(s).encode(\"utf-8\", errors=\"ignore\")[:max_bytes]\n",
    "    return [BYTE_OFFSET + int(x) for x in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ac17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "\n",
    "FEAT_DIM = 18\n",
    "\n",
    "def _features_for_strings(a: str, b: str) -> np.ndarray:\n",
    "    la, lb = len(a), len(b)\n",
    "    minl = max(1, min(la, lb))\n",
    "    maxl = max(1, max(la, lb))\n",
    "\n",
    "    ratio = difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    pref = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if x == y:\n",
    "            pref += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    suf = 0\n",
    "    for x, y in zip(a[::-1], b[::-1]):\n",
    "        if x == y:\n",
    "            suf += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    pref_r = pref / minl\n",
    "    suf_r = suf / minl\n",
    "\n",
    "    def bigrams(s: str) -> set:\n",
    "        return set([s[i:i+2] for i in range(len(s)-1)]) if len(s) >= 2 else set()\n",
    "\n",
    "    A = bigrams(a)\n",
    "    B = bigrams(b)\n",
    "    jac = (len(A & B) / max(1, len(A | B))) if (A or B) else 0.0\n",
    "\n",
    "    return np.array(\n",
    "        [la, lb, abs(la-lb), la/maxl, lb/maxl, ratio, pref_r, suf_r, jac],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "def compute_features(src_form: str, tgt_form: str) -> np.ndarray:\n",
    "    # raw\n",
    "    a_raw = normalize_form(src_form)\n",
    "    b_raw = normalize_form(tgt_form)\n",
    "    f_raw = _features_for_strings(a_raw, b_raw)\n",
    "\n",
    "    # romanized (склеивает кириллицу/латиницу и убирает диакритику)\n",
    "    a_rom = romanize_for_alignment(src_form)\n",
    "    b_rom = romanize_for_alignment(tgt_form)\n",
    "    f_rom = _features_for_strings(a_rom, b_rom)\n",
    "\n",
    "    return np.concatenate([f_raw, f_rom], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EncodedItem:\n",
    "    token_ids: List[int]\n",
    "    seg_ids: List[int]\n",
    "    feats: np.ndarray\n",
    "    label: int\n",
    "\n",
    "\n",
    "class LoanDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        *,\n",
    "        lang2id: Dict[str, int],\n",
    "        field2id: Dict[str, int],\n",
    "        cat2id: Dict[str, int],\n",
    "        max_src_bytes: int,\n",
    "        max_tgt_bytes: int,\n",
    "        max_len: int,\n",
    "        lang_base: int,\n",
    "        field_base: int,\n",
    "        cat_base: int,\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.lang2id = lang2id\n",
    "        self.field2id = field2id\n",
    "        self.cat2id = cat2id\n",
    "        self.max_src_bytes = max_src_bytes\n",
    "        self.max_tgt_bytes = max_tgt_bytes\n",
    "        self.max_len = max_len\n",
    "        self.lang_base = lang_base\n",
    "        self.field_base = field_base\n",
    "        self.cat_base = cat_base\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def encode_example(\n",
    "    self, src_form: str, tgt_form: str, tgt_lang: str, sem_field: str, sem_cat: str\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "        lang_id = self.lang2id.get(normalize_form(tgt_lang), 0)\n",
    "        field_id = self.field2id.get(normalize_form(sem_field), 0)\n",
    "        cat_id = self.cat2id.get(normalize_form(sem_cat), 0)\n",
    "\n",
    "        tokens = [\n",
    "            CLS_ID,\n",
    "            self.lang_base + lang_id,\n",
    "            self.field_base + field_id,\n",
    "            self.cat_base + cat_id,\n",
    "            SEP_ID,\n",
    "        ]\n",
    "        seg = [0, 0, 0, 0, 0]  # meta\n",
    "\n",
    "        src_raw = normalize_form(src_form)\n",
    "        src_rom = romanize_for_alignment(src_form)\n",
    "        tgt_raw = normalize_form(tgt_form)\n",
    "        tgt_rom = romanize_for_alignment(tgt_form)\n",
    "\n",
    "        src_raw_ids = to_bytes_ids(src_raw, self.max_src_bytes)\n",
    "        tokens += src_raw_ids + [SEP_ID]\n",
    "        seg += [1] * len(src_raw_ids) + [1]\n",
    "\n",
    "        src_rom_ids = to_bytes_ids(src_rom, self.max_src_bytes)\n",
    "        tokens += src_rom_ids + [SEP_ID]\n",
    "        seg += [2] * len(src_rom_ids) + [2]\n",
    "\n",
    "        tgt_raw_ids = to_bytes_ids(tgt_raw, self.max_tgt_bytes)\n",
    "        tokens += tgt_raw_ids + [SEP_ID]\n",
    "        seg += [3] * len(tgt_raw_ids) + [3]\n",
    "\n",
    "        tgt_rom_ids = to_bytes_ids(tgt_rom, self.max_tgt_bytes)\n",
    "        tokens += tgt_rom_ids + [SEP_ID]\n",
    "        seg += [4] * len(tgt_rom_ids) + [4]\n",
    "\n",
    "        tokens = tokens[: self.max_len]\n",
    "        seg = seg[: self.max_len]\n",
    "        return tokens, seg\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int) -> EncodedItem:\n",
    "        row = self.df.iloc[idx]\n",
    "        tok, seg = self.encode_example(\n",
    "            row[\"src_form\"],\n",
    "            row[\"tgt_form\"],\n",
    "            row[\"tgt_lang\"],\n",
    "            row[\"sem_field\"],\n",
    "            row[\"sem_cat\"],\n",
    "        )\n",
    "        feats = compute_features(row[\"src_form\"], row[\"tgt_form\"])\n",
    "        label = int(row[\"label_loan\"])\n",
    "        return EncodedItem(tok, seg, feats, label)\n",
    "\n",
    "\n",
    "def make_collate_fn(max_len: int):\n",
    "    def collate_fn(batch: List[EncodedItem]):\n",
    "        mx = min(max(len(x.token_ids) for x in batch), max_len)\n",
    "\n",
    "        token_ids = torch.full((len(batch), mx), PAD_ID, dtype=torch.long)\n",
    "        seg_ids = torch.zeros((len(batch), mx), dtype=torch.long)\n",
    "        feats = torch.zeros((len(batch), FEAT_DIM), dtype=torch.float32)\n",
    "        labels = torch.zeros((len(batch),), dtype=torch.float32)\n",
    "\n",
    "        for i, item in enumerate(batch):\n",
    "            t = item.token_ids[:mx]\n",
    "            s = item.seg_ids[:mx]\n",
    "            token_ids[i, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "            seg_ids[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "            feats[i] = torch.tensor(item.feats, dtype=torch.float32)\n",
    "            labels[i] = float(item.label)\n",
    "\n",
    "        pad_mask = token_ids == PAD_ID\n",
    "        return token_ids, seg_ids, pad_mask, feats, labels\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19221f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "\n",
    "class ByteCrossEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        vocab_size: int,\n",
    "        max_len: int,\n",
    "        d_model: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        ff_mult: int,\n",
    "        dropout: float,\n",
    "        feat_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
    "        self.seg_emb = nn.Embedding(5, d_model)  # 0 meta, 1 src_raw, 2 src_rom, 3 tgt_raw, 4 tgt_rom\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * ff_mult,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.cls_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model + feat_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        token_ids: torch.Tensor,\n",
    "        seg_ids: torch.Tensor,\n",
    "        pad_mask: torch.Tensor,\n",
    "        feats: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        B, T = token_ids.shape\n",
    "        pos = torch.arange(T, device=token_ids.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.tok_emb(token_ids) + self.seg_emb(seg_ids) + self.pos_emb(pos)\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "        x = self.ln(x)\n",
    "        cls = x[:, 0, :]\n",
    "        z = torch.cat([cls, feats], dim=1)\n",
    "        logits = self.cls_mlp(z).squeeze(1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a84b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Data construction\n",
    "# =========================\n",
    "\n",
    "def load_book2(book2_path: str) -> pd.DataFrame:\n",
    "    book2 = pd.read_excel(book2_path)\n",
    "\n",
    "    expected_cols = [\n",
    "        \"ID\",\n",
    "        \"Meaning\",\n",
    "        \"SemanticField\",\n",
    "        \"SemanticCategory\",\n",
    "        \"Target_Form\",\n",
    "        \"Target_Language_Name\",\n",
    "        \"Donor\",\n",
    "    ]\n",
    "    missing = [c for c in expected_cols if c not in book2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"В Книга2.xlsx отсутствуют ожидаемые столбцы: {missing}\")\n",
    "\n",
    "    book2 = book2.copy()\n",
    "    book2[\"src_form\"] = book2[\"Donor\"].map(normalize_form)\n",
    "    book2[\"tgt_form\"] = book2[\"Target_Form\"].map(normalize_form)\n",
    "    book2[\"tgt_lang\"] = book2[\"Target_Language_Name\"].astype(str).map(normalize_form)\n",
    "    book2[\"meaning\"] = book2[\"Meaning\"].astype(str)\n",
    "    book2[\"sem_field\"] = book2[\"SemanticField\"].astype(str).map(normalize_form)\n",
    "    book2[\"sem_cat\"] = book2[\"SemanticCategory\"].astype(str).map(normalize_form)\n",
    "    book2[\"label_loan\"] = 1\n",
    "    book2[\"source\"] = \"book2\"\n",
    "    book2[\"neg_type\"] = \"\"\n",
    "\n",
    "    book2 = book2[(book2[\"src_form\"] != \"\") & (book2[\"tgt_form\"] != \"\")].reset_index(drop=True)\n",
    "    return book2\n",
    "\n",
    "\n",
    "def sample_meta_from_book2(book2: pd.DataFrame, n: int, rng: random.Random) -> pd.DataFrame:\n",
    "    idx = rng.choices(range(len(book2)), k=n)\n",
    "    meta = book2.iloc[idx][[\"tgt_lang\", \"meaning\", \"sem_field\", \"sem_cat\"]].reset_index(drop=True)\n",
    "    return meta\n",
    "\n",
    "\n",
    "def build_cognate_negatives(book2: pd.DataFrame, cognates_path: Optional[str], rng: random.Random) -> pd.DataFrame:\n",
    "    cols = [\"src_form\", \"tgt_form\", \"tgt_lang\", \"meaning\", \"sem_field\", \"sem_cat\", \"label_loan\", \"neg_type\", \"source\"]\n",
    "    if not cognates_path or not os.path.exists(cognates_path):\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    raw = parse_pairs_txt(cognates_path)\n",
    "    raw = raw[raw[\"label\"] == 1].copy()\n",
    "    if raw.empty:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    a = raw.rename(columns={\"form1\": \"src_form\", \"form2\": \"tgt_form\"})[[\"src_form\", \"tgt_form\"]]\n",
    "    b = raw.rename(columns={\"form2\": \"src_form\", \"form1\": \"tgt_form\"})[[\"src_form\", \"tgt_form\"]]\n",
    "    cog = pd.concat([a, b], ignore_index=True)\n",
    "    cog = cog[(cog[\"src_form\"] != \"\") & (cog[\"tgt_form\"] != \"\")].reset_index(drop=True)\n",
    "\n",
    "    meta = sample_meta_from_book2(book2, len(cog), rng)\n",
    "    df = pd.concat([cog, meta], axis=1)\n",
    "    df[\"label_loan\"] = 0\n",
    "    df[\"neg_type\"] = \"cognate\"\n",
    "    df[\"source\"] = \"cognates_txt\"\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def build_txt2_negatives(book2: pd.DataFrame, txt2_path: Optional[str], rng: random.Random) -> pd.DataFrame:\n",
    "    cols = [\"src_form\", \"tgt_form\", \"tgt_lang\", \"meaning\", \"sem_field\", \"sem_cat\", \"label_loan\", \"neg_type\", \"source\"]\n",
    "    if not txt2_path or not os.path.exists(txt2_path):\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    raw = parse_pairs_txt(txt2_path)\n",
    "    raw = raw[raw[\"label\"] == 0].copy()\n",
    "    if raw.empty:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    a = raw.rename(columns={\"form1\": \"src_form\", \"form2\": \"tgt_form\"})[[\"src_form\", \"tgt_form\"]]\n",
    "    b = raw.rename(columns={\"form2\": \"src_form\", \"form1\": \"tgt_form\"})[[\"src_form\", \"tgt_form\"]]\n",
    "    negpairs = pd.concat([a, b], ignore_index=True)\n",
    "    negpairs = negpairs[(negpairs[\"src_form\"] != \"\") & (negpairs[\"tgt_form\"] != \"\")].reset_index(drop=True)\n",
    "\n",
    "    meta = sample_meta_from_book2(book2, len(negpairs), rng)\n",
    "    df = pd.concat([negpairs, meta], axis=1)\n",
    "    df[\"label_loan\"] = 0\n",
    "    df[\"neg_type\"] = \"txt2\"\n",
    "    df[\"source\"] = \"text2_txt\"\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def build_random_negatives(book2: pd.DataFrame, n: int, rng: random.Random) -> pd.DataFrame:\n",
    "    idx_t = rng.choices(range(len(book2)), k=n)\n",
    "    idx_s = rng.choices(range(len(book2)), k=n)\n",
    "\n",
    "    tgt = book2.iloc[idx_t][[\"tgt_form\", \"tgt_lang\", \"meaning\", \"sem_field\", \"sem_cat\"]].reset_index(drop=True)\n",
    "    src = book2.iloc[idx_s][[\"src_form\"]].reset_index(drop=True)\n",
    "    df = pd.concat([src, tgt], axis=1)\n",
    "\n",
    "    true_src = book2.iloc[idx_t][\"src_form\"].reset_index(drop=True)\n",
    "    mask_same = df[\"src_form\"].values == true_src.values\n",
    "    if mask_same.any():\n",
    "        repl_idx = rng.choices(range(len(book2)), k=int(mask_same.sum()))\n",
    "        df.loc[mask_same, \"src_form\"] = book2.iloc[repl_idx][\"src_form\"].values\n",
    "\n",
    "    df[\"label_loan\"] = 0\n",
    "    df[\"neg_type\"] = \"random\"\n",
    "    df[\"source\"] = \"generated_random\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_hard_negatives(book2: pd.DataFrame, n: int, rng: random.Random, sample_pool: int = 80) -> pd.DataFrame:\n",
    "    donors = book2[\"src_form\"].tolist()\n",
    "    rows = []\n",
    "    for _ in range(n):\n",
    "        j = rng.randrange(len(book2))\n",
    "        tgt_form = book2.iloc[j][\"tgt_form\"]\n",
    "        true_src = book2.iloc[j][\"src_form\"]\n",
    "        meta = book2.iloc[j][[\"tgt_lang\", \"meaning\", \"sem_field\", \"sem_cat\"]].to_dict()\n",
    "\n",
    "        cand_idx = rng.sample(range(len(donors)), k=min(sample_pool, len(donors)))\n",
    "        best = None\n",
    "        best_score = -1.0\n",
    "        for ci in cand_idx:\n",
    "            s = donors[ci]\n",
    "            if s == true_src:\n",
    "                continue\n",
    "            score = difflib.SequenceMatcher(None, s, tgt_form).ratio()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best = s\n",
    "\n",
    "        if best is None:\n",
    "            best = donors[rng.randrange(len(donors))]\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"src_form\": best,\n",
    "                \"tgt_form\": tgt_form,\n",
    "                \"tgt_lang\": meta[\"tgt_lang\"],\n",
    "                \"meaning\": meta[\"meaning\"],\n",
    "                \"sem_field\": meta[\"sem_field\"],\n",
    "                \"sem_cat\": meta[\"sem_cat\"],\n",
    "                \"label_loan\": 0,\n",
    "                \"neg_type\": \"hard_similar\",\n",
    "                \"source\": \"generated_hard\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0deada22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_dataset(\n",
    "    book2: pd.DataFrame,\n",
    "    cognates_path: Optional[str],\n",
    "    txt2_path: Optional[str],\n",
    "    seed: int,\n",
    "    neg_frac_cognate: float,\n",
    "    neg_frac_random: float,\n",
    "    neg_frac_hard: float,\n",
    ") -> pd.DataFrame:\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    pos_df = book2[\n",
    "        [\"src_form\", \"tgt_form\", \"tgt_lang\", \"meaning\", \"sem_field\", \"sem_cat\", \"label_loan\", \"neg_type\", \"source\"]\n",
    "    ].copy()\n",
    "\n",
    "    cog_df = build_cognate_negatives(book2, cognates_path, rng)\n",
    "    txt2_df = build_txt2_negatives(book2, txt2_path, rng)\n",
    "\n",
    "    available = pd.concat([cog_df, txt2_df], ignore_index=True)\n",
    "    available = available.drop_duplicates(subset=[\"src_form\", \"tgt_form\", \"tgt_lang\", \"sem_field\", \"sem_cat\"])\n",
    "\n",
    "    N_pos = len(pos_df)\n",
    "    n_need = N_pos\n",
    "\n",
    "    if len(available) >= n_need:\n",
    "        neg_df = available.sample(n=n_need, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        neg_df = available.copy().reset_index(drop=True)\n",
    "        remaining = n_need - len(neg_df)\n",
    "\n",
    "        n_cog_target = int(remaining * neg_frac_cognate)\n",
    "        n_rand_target = int(remaining * neg_frac_random)\n",
    "        n_hard_target = remaining - n_cog_target - n_rand_target\n",
    "\n",
    "        # если когнатов мало, переносим дефицит в random/hard\n",
    "        if len(cog_df) < n_cog_target:\n",
    "            deficit = n_cog_target - len(cog_df)\n",
    "            n_cog_target = len(cog_df)\n",
    "            n_rand_target += deficit // 2\n",
    "            n_hard_target += deficit - deficit // 2\n",
    "\n",
    "        if n_cog_target > 0 and len(cog_df) > 0:\n",
    "            extra_cog = cog_df.sample(\n",
    "                n=n_cog_target,\n",
    "                replace=(len(cog_df) < n_cog_target),\n",
    "                random_state=seed,\n",
    "            )\n",
    "            neg_df = pd.concat([neg_df, extra_cog], ignore_index=True)\n",
    "\n",
    "        if n_rand_target > 0:\n",
    "            neg_df = pd.concat([neg_df, build_random_negatives(book2, n_rand_target, rng)], ignore_index=True)\n",
    "\n",
    "        if n_hard_target > 0:\n",
    "            neg_df = pd.concat([neg_df, build_hard_negatives(book2, n_hard_target, rng)], ignore_index=True)\n",
    "\n",
    "        neg_df = neg_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    data = pd.concat([pos_df, neg_df], ignore_index=True).dropna()\n",
    "    data[\"src_form_norm\"] = data[\"src_form\"].map(normalize_form)\n",
    "    data[\"tgt_form_norm\"] = data[\"tgt_form\"].map(normalize_form)\n",
    "    data = data[(data[\"src_form_norm\"] != \"\") & (data[\"tgt_form_norm\"] != \"\")].reset_index(drop=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def group_split_by_tgt_form(\n",
    "    data: pd.DataFrame, seed: int, test_size: float = 0.15, val_size: float = 0.15\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    groups = data[\"tgt_form_norm\"].values\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    train_idx, test_idx = next(gss.split(data, groups=groups))\n",
    "    train_df = data.iloc[train_idx].reset_index(drop=True)\n",
    "    test_df = data.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=seed)\n",
    "    tr_idx, va_idx = next(gss2.split(train_df, groups=train_df[\"tgt_form_norm\"].values))\n",
    "    tr_df = train_df.iloc[tr_idx].reset_index(drop=True)\n",
    "    va_df = train_df.iloc[va_idx].reset_index(drop=True)\n",
    "    return tr_df, va_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1db004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Train / Eval\n",
    "# =========================\n",
    "\n",
    "def evaluate(loader: DataLoader, model: nn.Module, device: torch.device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    all_y: List[np.ndarray] = []\n",
    "    all_p: List[np.ndarray] = []\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token_ids, seg_ids, pad_mask, feats, labels in loader:\n",
    "            token_ids = token_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            pad_mask = pad_mask.to(device)\n",
    "            feats = feats.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(token_ids, seg_ids, pad_mask, feats)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            total_loss += float(loss.item()) * labels.size(0)\n",
    "            n += labels.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            all_p.append(probs)\n",
    "            all_y.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    y = np.concatenate(all_y) if all_y else np.array([], dtype=np.float32)\n",
    "    p = np.concatenate(all_p) if all_p else np.array([], dtype=np.float32)\n",
    "    pred = (p >= 0.5).astype(int) if len(p) else np.array([], dtype=int)\n",
    "\n",
    "    acc = float(accuracy_score(y, pred)) if len(y) else 0.0\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y, pred, average=\"binary\", zero_division=0) if len(y) else (0,0,0,None)\n",
    "    try:\n",
    "        auc = float(roc_auc_score(y, p)) if len(y) else float(\"nan\")\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, n),\n",
    "        \"acc\": acc,\n",
    "        \"precision\": float(pr),\n",
    "        \"recall\": float(rc),\n",
    "        \"f1\": float(f1),\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    *,\n",
    "    model: nn.Module,\n",
    "    tr_loader: DataLoader,\n",
    "    va_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    outdir: Path,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    grad_clip_norm: float,\n",
    "    patience: int,\n",
    "    use_amp: bool,\n",
    ") -> Dict[str, object]:\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scaler = GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    bad_epochs = 0\n",
    "    best_path = outdir / \"best_model.pt\"\n",
    "    metrics_path = outdir / \"train_metrics.jsonl\"\n",
    "\n",
    "    with metrics_path.open(\"w\", encoding=\"utf-8\") as mf:\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            n = 0\n",
    "\n",
    "            for token_ids, seg_ids, pad_mask, feats, labels in tr_loader:\n",
    "                token_ids = token_ids.to(device)\n",
    "                seg_ids = seg_ids.to(device)\n",
    "                pad_mask = pad_mask.to(device)\n",
    "                feats = feats.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with autocast(enabled=(use_amp and device.type == \"cuda\")):\n",
    "                    logits = model(token_ids, seg_ids, pad_mask, feats)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip_norm is not None and grad_clip_norm > 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                total_loss += float(loss.item()) * labels.size(0)\n",
    "                n += labels.size(0)\n",
    "\n",
    "            train_loss = total_loss / max(1, n)\n",
    "            val_metrics = evaluate(va_loader, model, device)\n",
    "\n",
    "            log_row = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": float(train_loss),\n",
    "                **{f\"val_{k}\": float(v) if isinstance(v, (int, float, np.floating)) else v for k, v in val_metrics.items()},\n",
    "            }\n",
    "            mf.write(json.dumps(log_row, ensure_ascii=False) + \"\\n\")\n",
    "            mf.flush()\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"train_loss={train_loss:.4f} | \"\n",
    "                f\"val_loss={val_metrics['loss']:.4f} \"\n",
    "                f\"acc={val_metrics['acc']:.4f} \"\n",
    "                f\"f1={val_metrics['f1']:.4f} \"\n",
    "                f\"auc={val_metrics['auc']:.4f}\"\n",
    "            )\n",
    "\n",
    "            if val_metrics[\"f1\"] > best_f1 + 1e-5:\n",
    "                best_f1 = val_metrics[\"f1\"]\n",
    "                bad_epochs = 0\n",
    "                torch.save({\"model_state\": model.state_dict()}, best_path)\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= patience:\n",
    "                    print(f\"Early stopping: no F1 improvement for {patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "    ckpt = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    return {\"best_f1\": float(best_f1), \"best_model_path\": str(best_path)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dd614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Save / Load artifacts\n",
    "# =========================\n",
    "\n",
    "def save_artifacts(\n",
    "    *,\n",
    "    outdir: Path,\n",
    "    config: Dict[str, object],\n",
    "    lang2id: Dict[str, int],\n",
    "    field2id: Dict[str, int],\n",
    "    cat2id: Dict[str, int],\n",
    ") -> None:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    (outdir / \"config.json\").write_text(json.dumps(config, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    maps = {\"lang2id\": lang2id, \"field2id\": field2id, \"cat2id\": cat2id}\n",
    "    (outdir / \"meta_maps.json\").write_text(json.dumps(maps, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    tokenizer_cfg = {\n",
    "        \"normalization\": {\n",
    "            \"unicode\": \"NFKC\",\n",
    "            \"apostrophes\": [\"’\", \"ʻ\", \"`\", \"´\"],\n",
    "            \"apostrophe_repl\": \"'\",\n",
    "            \"collapse_spaces\": True,\n",
    "        }\n",
    "    }\n",
    "    (outdir / \"tokenizer_config.json\").write_text(json.dumps(tokenizer_cfg, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    env = {\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"torch\": torch.__version__,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "    }\n",
    "    (outdir / \"env.json\").write_text(json.dumps(env, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def load_model_for_inference(artifact_dir: str, device: Optional[str] = None) -> Tuple[ByteCrossEncoder, Dict[str, object]]:\n",
    "    ad = Path(artifact_dir)\n",
    "    cfg = json.loads((ad / \"config.json\").read_text(encoding=\"utf-8\"))\n",
    "    maps = json.loads((ad / \"meta_maps.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    model = ByteCrossEncoder(\n",
    "        vocab_size=int(cfg[\"vocab_size\"]),\n",
    "        max_len=int(cfg[\"max_len\"]),\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        ff_mult=int(cfg[\"ff_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        feat_dim=int(cfg[\"feat_dim\"]),\n",
    "    )\n",
    "\n",
    "    ckpt = torch.load(ad / \"best_model.pt\", map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    if device is None:\n",
    "        device_t = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device_t = torch.device(device)\n",
    "    model.to(device_t)\n",
    "\n",
    "    runtime = {\n",
    "        \"config\": cfg,\n",
    "        \"maps\": maps,\n",
    "        \"device\": str(device_t),\n",
    "    }\n",
    "    return model, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23bfcf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "Book2 positives: 20886\n",
      "Total rows: 41772\n",
      "Label distribution: {1: 20886, 0: 20886}\n",
      "Train/Val/Test: 33897 3719 4156\n",
      "VOCAB_SIZE: 653 MAX_LEN: 265\n",
      "Model params: 6761473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n",
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.3027 | val_loss=0.2279 acc=0.9064 f1=0.9089 auc=0.9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_loss=0.1725 | val_loss=0.1532 acc=0.9465 f1=0.9453 auc=0.9851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train_loss=0.1364 | val_loss=0.1377 acc=0.9473 f1=0.9448 auc=0.9883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train_loss=0.1077 | val_loss=0.1147 acc=0.9602 f1=0.9590 auc=0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train_loss=0.0819 | val_loss=0.1073 acc=0.9640 f1=0.9627 auc=0.9921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train_loss=0.0713 | val_loss=0.1217 acc=0.9572 f1=0.9546 auc=0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train_loss=0.0570 | val_loss=0.1099 acc=0.9659 f1=0.9652 auc=0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train_loss=0.0468 | val_loss=0.1289 acc=0.9648 f1=0.9629 auc=0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | train_loss=0.0361 | val_loss=0.1207 acc=0.9642 f1=0.9627 auc=0.9924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train_loss=0.0331 | val_loss=0.1422 acc=0.9607 f1=0.9585 auc=0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train_loss=0.0270 | val_loss=0.1277 acc=0.9653 f1=0.9644 auc=0.9935\n",
      "Early stopping: no F1 improvement for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/4122921811.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(best_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: {'loss': 0.0917612046820826, 'acc': 0.9646294513955727, 'precision': 0.9415520628683693, 'recall': 0.9856041131105399, 'f1': 0.9630746043707611, 'auc': 0.9960412938811435}\n",
      "Saved artifacts to: loanword_artifacts\n",
      "Files: ['best_model.pt', 'config.json', 'env.json', 'meta_maps.json', 'summary.json', 'tokenizer_config.json', 'train_metrics.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# CONFIG (edit here)\n",
    "# =====================\n",
    "BOOK2_PATH = \"/Users/oksanagoncarova/Desktop/ipynb/Книга2.xlsx\"          \n",
    "COGNATES_TXT_PATH = \"/Users/oksanagoncarova/Desktop/ipynb/test_pos.txt\"     \n",
    "NEGATIVES_TXT2_PATH = \"/Users/oksanagoncarova/Desktop/ipynb/test_neg.txt\"                   \n",
    "\n",
    "OUTDIR = \"loanword_artifacts\"\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "LR = 2e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "PATIENCE = 4\n",
    "USE_AMP = True  # работает только при CUDA\n",
    "\n",
    "MAX_SRC_BYTES = 64\n",
    "MAX_TGT_BYTES = 64\n",
    "\n",
    "D_MODEL = 256\n",
    "N_LAYERS = 8\n",
    "N_HEADS = 8\n",
    "FF_MULT = 4\n",
    "DROPOUT = 0.10\n",
    "\n",
    "NEG_FRAC_COGNATE = 0.50\n",
    "NEG_FRAC_RANDOM  = 0.25\n",
    "NEG_FRAC_HARD    = 0.25\n",
    "\n",
    "FAST_DEV_RUN = False\n",
    "FAST_ROWS = 5000\n",
    "\n",
    "# =====================\n",
    "# RUN\n",
    "# =====================\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", device)\n",
    "\n",
    "# Проверка файлов\n",
    "assert os.path.exists(BOOK2_PATH), f\"Не найден файл: {BOOK2_PATH}\"\n",
    "assert os.path.exists(COGNATES_TXT_PATH), f\"Не найден файл: {COGNATES_TXT_PATH}\"\n",
    "if NEGATIVES_TXT2_PATH is not None:\n",
    "    assert os.path.exists(NEGATIVES_TXT2_PATH), f\"Не найден файл: {NEGATIVES_TXT2_PATH}\"\n",
    "\n",
    "outdir = Path(OUTDIR)\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Позитивы\n",
    "book2 = load_book2(BOOK2_PATH)\n",
    "print(\"Book2 positives:\", len(book2))\n",
    "\n",
    "# 2) Общий датасет (позитивы + негативы)\n",
    "data = assemble_dataset(\n",
    "    book2=book2,\n",
    "    cognates_path=COGNATES_TXT_PATH,\n",
    "    txt2_path=NEGATIVES_TXT2_PATH,\n",
    "    seed=SEED,\n",
    "    neg_frac_cognate=NEG_FRAC_COGNATE,\n",
    "    neg_frac_random=NEG_FRAC_RANDOM,\n",
    "    neg_frac_hard=NEG_FRAC_HARD,\n",
    ")\n",
    "\n",
    "if FAST_DEV_RUN:\n",
    "    data = data.sample(n=min(len(data), FAST_ROWS), random_state=SEED).reset_index(drop=True)\n",
    "    print(\"[FAST_DEV_RUN] Using rows:\", len(data))\n",
    "\n",
    "print(\"Total rows:\", len(data))\n",
    "print(\"Label distribution:\", data[\"label_loan\"].value_counts().to_dict())\n",
    "\n",
    "# 3) Split без утечек (группировка по tgt_form)\n",
    "tr_df, va_df, te_df = group_split_by_tgt_form(data, seed=SEED, test_size=0.1, val_size=0.1)\n",
    "print(\"Train/Val/Test:\", len(tr_df), len(va_df), len(te_df))\n",
    "\n",
    "# 4) Словари метаданных\n",
    "lang2id = build_id_map(data[\"tgt_lang\"])\n",
    "field2id = build_id_map(data[\"sem_field\"])\n",
    "cat2id = build_id_map(data[\"sem_cat\"])\n",
    "\n",
    "lang_base = META_OFFSET\n",
    "field_base = lang_base + len(lang2id)\n",
    "cat_base = field_base + len(field2id)\n",
    "vocab_size = cat_base + len(cat2id)\n",
    "\n",
    "max_len = 1 + 3 + 1 + 2 * (MAX_SRC_BYTES + 1) + 2 * (MAX_TGT_BYTES + 1)\n",
    "\n",
    "\n",
    "print(\"VOCAB_SIZE:\", vocab_size, \"MAX_LEN:\", max_len)\n",
    "\n",
    "# 5) DataLoaders\n",
    "collate_fn = make_collate_fn(max_len)\n",
    "\n",
    "tr_ds = LoanDataset(\n",
    "    tr_df,\n",
    "    lang2id=lang2id,\n",
    "    field2id=field2id,\n",
    "    cat2id=cat2id,\n",
    "    max_src_bytes=MAX_SRC_BYTES,\n",
    "    max_tgt_bytes=MAX_TGT_BYTES,\n",
    "    max_len=max_len,\n",
    "    lang_base=lang_base,\n",
    "    field_base=field_base,\n",
    "    cat_base=cat_base,\n",
    ")\n",
    "va_ds = LoanDataset(\n",
    "    va_df,\n",
    "    lang2id=lang2id,\n",
    "    field2id=field2id,\n",
    "    cat2id=cat2id,\n",
    "    max_src_bytes=MAX_SRC_BYTES,\n",
    "    max_tgt_bytes=MAX_TGT_BYTES,\n",
    "    max_len=max_len,\n",
    "    lang_base=lang_base,\n",
    "    field_base=field_base,\n",
    "    cat_base=cat_base,\n",
    ")\n",
    "te_ds = LoanDataset(\n",
    "    te_df,\n",
    "    lang2id=lang2id,\n",
    "    field2id=field2id,\n",
    "    cat2id=cat2id,\n",
    "    max_src_bytes=MAX_SRC_BYTES,\n",
    "    max_tgt_bytes=MAX_TGT_BYTES,\n",
    "    max_len=max_len,\n",
    "    lang_base=lang_base,\n",
    "    field_base=field_base,\n",
    "    cat_base=cat_base,\n",
    ")\n",
    "\n",
    "tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n",
    "va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "te_loader = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# 6) Model\n",
    "model = ByteCrossEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    ff_mult=FF_MULT,\n",
    "    dropout=DROPOUT,\n",
    "    feat_dim=FEAT_DIM,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Model params:\", n_params)\n",
    "\n",
    "# 7) Train\n",
    "# В ноутбуке USE_AMP имеет смысл только при CUDA\n",
    "use_amp_effective = bool(USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "train_info = train_loop(\n",
    "    model=model,\n",
    "    tr_loader=tr_loader,\n",
    "    va_loader=va_loader,\n",
    "    device=device,\n",
    "    outdir=outdir,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    grad_clip_norm=GRAD_CLIP,\n",
    "    patience=PATIENCE,\n",
    "    use_amp=use_amp_effective,\n",
    ")\n",
    "\n",
    "# 8) Test\n",
    "test_metrics = evaluate(te_loader, model, device)\n",
    "print(\"TEST:\", test_metrics)\n",
    "\n",
    "# 9) Save artifacts\n",
    "config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"max_len\": max_len,\n",
    "    \"d_model\": D_MODEL,\n",
    "    \"n_layers\": N_LAYERS,\n",
    "    \"n_heads\": N_HEADS,\n",
    "    \"ff_mult\": FF_MULT,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"feat_dim\": FEAT_DIM,\n",
    "    \"pad_id\": PAD_ID,\n",
    "    \"cls_id\": CLS_ID,\n",
    "    \"sep_id\": SEP_ID,\n",
    "    \"byte_offset\": BYTE_OFFSET,\n",
    "    \"meta_offset\": META_OFFSET,\n",
    "    \"lang_base\": lang_base,\n",
    "    \"field_base\": field_base,\n",
    "    \"cat_base\": cat_base,\n",
    "    \"max_src_bytes\": MAX_SRC_BYTES,\n",
    "    \"max_tgt_bytes\": MAX_TGT_BYTES,\n",
    "    \"seed\": SEED,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": LR,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"grad_clip\": GRAD_CLIP,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"use_amp\": use_amp_effective,\n",
    "    \"neg_fracs\": {\n",
    "        \"cognate\": NEG_FRAC_COGNATE,\n",
    "        \"random\": NEG_FRAC_RANDOM,\n",
    "        \"hard\": NEG_FRAC_HARD,\n",
    "    },\n",
    "}\n",
    "\n",
    "save_artifacts(\n",
    "    outdir=outdir,\n",
    "    config=config,\n",
    "    lang2id=lang2id,\n",
    "    field2id=field2id,\n",
    "    cat2id=cat2id,\n",
    ")\n",
    "\n",
    "summary = {\n",
    "    \"train_info\": train_info,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"rows\": {\"total\": int(len(data)), \"train\": int(len(tr_df)), \"val\": int(len(va_df)), \"test\": int(len(te_df))},\n",
    "    \"device\": str(device),\n",
    "    \"model_params\": int(n_params),\n",
    "}\n",
    "(outdir / \"summary.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved artifacts to:\", str(outdir))\n",
    "print(\"Files:\", [p.name for p in sorted(outdir.glob('*'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e869ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shíhuī  ->  шохой :  p_loan=0.999378\n",
      "tuge  ->  tuɛ :  p_loan=0.938874\n",
      "yàngzi  ->  янза :  p_loan=0.996503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_2188/1015904495.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(os.path.join(ad, \"best_model.pt\"), map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ---------- Загрузка модели из артефактов (если нужно) ----------\n",
    "def load_loanword_model(artifact_dir: str, device: str | None = None):\n",
    "    \"\"\"\n",
    "    Ожидает в artifact_dir файлы:\n",
    "      - config.json\n",
    "      - meta_maps.json\n",
    "      - best_model.pt\n",
    "    Возвращает: (model, runtime_dict)\n",
    "    \"\"\"\n",
    "    ad = artifact_dir\n",
    "    cfg = json.loads(open(os.path.join(ad, \"config.json\"), \"r\", encoding=\"utf-8\").read())\n",
    "    maps = json.loads(open(os.path.join(ad, \"meta_maps.json\"), \"r\", encoding=\"utf-8\").read())\n",
    "\n",
    "    if device is None:\n",
    "        device_t = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device_t = torch.device(device)\n",
    "\n",
    "    model = ByteCrossEncoder(\n",
    "        vocab_size=int(cfg[\"vocab_size\"]),\n",
    "        max_len=int(cfg[\"max_len\"]),\n",
    "        d_model=int(cfg[\"d_model\"]),\n",
    "        n_layers=int(cfg[\"n_layers\"]),\n",
    "        n_heads=int(cfg[\"n_heads\"]),\n",
    "        ff_mult=int(cfg[\"ff_mult\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "        feat_dim=int(cfg[\"feat_dim\"]),\n",
    "    )\n",
    "    ckpt = torch.load(os.path.join(ad, \"best_model.pt\"), map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.to(device_t)\n",
    "    model.eval()\n",
    "\n",
    "    runtime = {\n",
    "        \"device\": device_t,\n",
    "        \"config\": cfg,\n",
    "        \"maps\": maps,\n",
    "    }\n",
    "    return model, runtime\n",
    "\n",
    "\n",
    "# ---------- Предсказание вероятностей ----------\n",
    "@torch.no_grad()\n",
    "def predict_loan_probabilities(\n",
    "    pairs,\n",
    "    *,\n",
    "    artifact_dir: str | None = None,\n",
    "    model: torch.nn.Module | None = None,\n",
    "    runtime: dict | None = None,\n",
    "    tgt_lang: str | None = None,\n",
    "    sem_field: str | None = None,\n",
    "    sem_cat: str | None = None,\n",
    "    batch_size: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    pairs: list[tuple[str,str]]  -> [(src_form, tgt_form), ...]\n",
    "    Возвращает: list[float] вероятностей заимствования в том же порядке.\n",
    "\n",
    "    Варианты использования:\n",
    "      A) Если модель уже в памяти:\n",
    "         probs = predict_loan_probabilities(pairs, model=model, runtime=runtime, tgt_lang=\"...\", sem_field=\"...\", sem_cat=\"...\")\n",
    "      B) Если есть сохранённые артефакты:\n",
    "         probs = predict_loan_probabilities(pairs, artifact_dir=\".../artifacts\", tgt_lang=\"...\", sem_field=\"...\", sem_cat=\"...\")\n",
    "\n",
    "    Если tgt_lang/sem_field/sem_cat не заданы, будут использованы пустые строки.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        if artifact_dir is None:\n",
    "            raise ValueError(\"Нужно передать либо model+runtime, либо artifact_dir с артефактами.\")\n",
    "        model, runtime = load_loanword_model(artifact_dir)\n",
    "\n",
    "    if runtime is None:\n",
    "        raise ValueError(\"runtime обязателен (должен содержать config и maps).\")\n",
    "\n",
    "    device = runtime[\"device\"] if \"device\" in runtime else next(model.parameters()).device\n",
    "    cfg = runtime[\"config\"]\n",
    "    maps = runtime[\"maps\"]\n",
    "\n",
    "    lang2id = maps[\"lang2id\"]\n",
    "    field2id = maps[\"field2id\"]\n",
    "    cat2id = maps[\"cat2id\"]\n",
    "\n",
    "    PAD_ID = int(cfg[\"pad_id\"])\n",
    "    CLS_ID = int(cfg[\"cls_id\"])\n",
    "    SEP_ID = int(cfg[\"sep_id\"])\n",
    "    BYTE_OFFSET = int(cfg[\"byte_offset\"])\n",
    "    LANG_BASE = int(cfg[\"lang_base\"])\n",
    "    FIELD_BASE = int(cfg[\"field_base\"])\n",
    "    CAT_BASE = int(cfg[\"cat_base\"])\n",
    "\n",
    "    MAX_LEN = int(cfg[\"max_len\"])\n",
    "    MAX_SRC_BYTES = int(cfg[\"max_src_bytes\"])\n",
    "    MAX_TGT_BYTES = int(cfg[\"max_tgt_bytes\"])\n",
    "\n",
    "    def _to_bytes_ids(s: str, max_bytes: int):\n",
    "        b = normalize_form(s).encode(\"utf-8\", errors=\"ignore\")[:max_bytes]\n",
    "        return [BYTE_OFFSET + int(x) for x in b]\n",
    "\n",
    "    def _encode(src_form: str, tgt_form: str, _tgt_lang: str, _sem_field: str, _sem_cat: str):\n",
    "        # Важно: при отсутствии ключа используется 0 (как было при обучении)\n",
    "        lang_id = lang2id.get(normalize_form(_tgt_lang), 0)\n",
    "        field_id = field2id.get(normalize_form(_sem_field), 0)\n",
    "        cat_id = cat2id.get(normalize_form(_sem_cat), 0)\n",
    "\n",
    "        tokens = [CLS_ID, LANG_BASE + lang_id, FIELD_BASE + field_id, CAT_BASE + cat_id, SEP_ID]\n",
    "        seg =    [0,      0,               0,                0,              0]\n",
    "\n",
    "        src_ids = _to_bytes_ids(src_form, MAX_SRC_BYTES)\n",
    "        tokens += src_ids + [SEP_ID]\n",
    "        seg += [1] * len(src_ids) + [1]\n",
    "\n",
    "        tgt_ids = _to_bytes_ids(tgt_form, MAX_TGT_BYTES)\n",
    "        tokens += tgt_ids + [SEP_ID]\n",
    "        seg += [2] * len(tgt_ids) + [2]\n",
    "\n",
    "        tokens = tokens[:MAX_LEN]\n",
    "        seg = seg[:MAX_LEN]\n",
    "        return tokens, seg\n",
    "\n",
    "    # метаданные по умолчанию\n",
    "    _tgt_lang = \"\" if tgt_lang is None else tgt_lang\n",
    "    _sem_field = \"\" if sem_field is None else sem_field\n",
    "    _sem_cat = \"\" if sem_cat is None else sem_cat\n",
    "\n",
    "    probs_out = []\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i : i + batch_size]\n",
    "\n",
    "        tok_list, seg_list, feat_list = [], [], []\n",
    "        maxlen = 0\n",
    "        for src, tgt in batch:\n",
    "            tok, seg = _encode(src, tgt, _tgt_lang, _sem_field, _sem_cat)\n",
    "            tok_list.append(tok)\n",
    "            seg_list.append(seg)\n",
    "            feat_list.append(compute_features(src, tgt))\n",
    "            maxlen = max(maxlen, len(tok))\n",
    "\n",
    "        maxlen = min(maxlen, MAX_LEN)\n",
    "\n",
    "        token_ids = torch.full((len(batch), maxlen), PAD_ID, dtype=torch.long)\n",
    "        seg_ids = torch.zeros((len(batch), maxlen), dtype=torch.long)\n",
    "        feats = torch.tensor(np.stack(feat_list, axis=0), dtype=torch.float32)\n",
    "\n",
    "        for r in range(len(batch)):\n",
    "            t = tok_list[r][:maxlen]\n",
    "            s = seg_list[r][:maxlen]\n",
    "            token_ids[r, :len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "            seg_ids[r, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "\n",
    "        pad_mask = token_ids.eq(PAD_ID)\n",
    "\n",
    "        token_ids = token_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        pad_mask = pad_mask.to(device)\n",
    "        feats = feats.to(device)\n",
    "\n",
    "        logits = model(token_ids, seg_ids, pad_mask, feats)\n",
    "        p = torch.sigmoid(logits).detach().cpu().numpy().tolist()\n",
    "        probs_out.extend(p)\n",
    "\n",
    "    return probs_out\n",
    "\n",
    "\n",
    "# ---------- Пример ----------\n",
    "pairs = [(\"shíhuī\", \"шохой\"), (\"tuge\", \"tuɛ\"), (\"yàngzi\", \"янза\")]\n",
    "\n",
    "# Вариант A: если модель уже обучена в ноутбуке и лежит в переменных model, outdir и т.п.\n",
    "# probs = predict_loan_probabilities(pairs, model=model, runtime={\"device\": next(model.parameters()).device, \"config\": config, \"maps\": {\"lang2id\": lang2id, \"field2id\": field2id, \"cat2id\": cat2id}})\n",
    "\n",
    "# Вариант B: если есть сохранённые артефакты (путь замените на ваш каталог)\n",
    "# probs = predict_loan_probabilities(pairs, artifact_dir=OUTDIR)\n",
    "\n",
    "probs = predict_loan_probabilities(pairs, artifact_dir=OUTDIR)\n",
    "\n",
    "for (src, tgt), p in zip(pairs, probs):\n",
    "    print(f\"{src}  ->  {tgt} :  p_loan={p:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
